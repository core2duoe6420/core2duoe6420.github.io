<!doctype html><html lang=en-us><head><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"OpenWrt Hardware Flow Offload Causing MAC Address Caching","datePublished":"2025-11-18T00:00:00Z","dateModified":"2025-11-18T00:00:00Z","author":{"@type":"Person","name":"Alex King","image":"https://res.cloudinary.com/core2duoe6420/image/upload/v1643906248/DSC_5117_wviusa.jpg"},"mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/blog.hljin.net\/en-us\/2025\/11\/flowtable-mac-issue\/"},"publisher":{"@type":"Organization","name":"Alex King's blog","logo":{"@type":"ImageObject","url":"https://res.cloudinary.com/core2duoe6420/image/upload/v1643906248/DSC_5117_wviusa.jpg"}},"description":" This article is translated from Chinese to English by ChatGPT. There might be errors.\nRan into yet another pitfall. This time it happened when I tried to migrate my original Proxmox VM ImmortalWrt (a build of OpenWrt) into Docker while keeping the IP address unchanged. This ImmortalWrt instance is running my WireGuard service. The migration itself went smoothly. After starting the Docker container, ping worked fine, and my phone could connect to WireGuard. Only one always-on 24/7 node stubbornly refused to connect: no handshake, WireGuard showed 0 KB received, not a single packet came in.\n","keywords":[]}</script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Hugo 0.111.3 with theme Tranquilpeak 0.5.3-BETA"><meta name=author content="Alex King"><meta name=keywords content><meta name=description content="
  This article is translated from Chinese to English by ChatGPT. There might be errors.

Ran into yet another pitfall. This time it happened when I tried to migrate my original Proxmox VM ImmortalWrt (a build of OpenWrt) into Docker while keeping the IP address unchanged. This ImmortalWrt instance is running my WireGuard service. The migration itself went smoothly. After starting the Docker container, ping worked fine, and my phone could connect to WireGuard. Only one always-on 24/7 node stubbornly refused to connect: no handshake, WireGuard showed 0 KB received, not a single packet came in."><meta property="og:description" content="
  This article is translated from Chinese to English by ChatGPT. There might be errors.

Ran into yet another pitfall. This time it happened when I tried to migrate my original Proxmox VM ImmortalWrt (a build of OpenWrt) into Docker while keeping the IP address unchanged. This ImmortalWrt instance is running my WireGuard service. The migration itself went smoothly. After starting the Docker container, ping worked fine, and my phone could connect to WireGuard. Only one always-on 24/7 node stubbornly refused to connect: no handshake, WireGuard showed 0 KB received, not a single packet came in."><meta property="og:type" content="article"><meta property="og:title" content="OpenWrt Hardware Flow Offload Causing MAC Address Caching"><meta name=twitter:title content="OpenWrt Hardware Flow Offload Causing MAC Address Caching"><meta property="og:url" content="https://blog.hljin.net/en-us/2025/11/flowtable-mac-issue/"><meta property="twitter:url" content="https://blog.hljin.net/en-us/2025/11/flowtable-mac-issue/"><meta property="og:site_name" content="Alex King's blog"><meta property="og:description" content="
  This article is translated from Chinese to English by ChatGPT. There might be errors.

Ran into yet another pitfall. This time it happened when I tried to migrate my original Proxmox VM ImmortalWrt (a build of OpenWrt) into Docker while keeping the IP address unchanged. This ImmortalWrt instance is running my WireGuard service. The migration itself went smoothly. After starting the Docker container, ping worked fine, and my phone could connect to WireGuard. Only one always-on 24/7 node stubbornly refused to connect: no handshake, WireGuard showed 0 KB received, not a single packet came in."><meta name=twitter:description content="
  This article is translated from Chinese to English by ChatGPT. There might be errors.

Ran into yet another pitfall. This time it happened when I tried to migrate my original Proxmox VM ImmortalWrt (a build of OpenWrt) into Docker while keeping the IP address unchanged. This ImmortalWrt instance is running my WireGuard service. The migration itself went smoothly. After starting the Docker container, ping worked fine, and my phone could connect to WireGuard. Only one always-on 24/7 node stubbornly refused to connect: no handshake, WireGuard showed 0 KB received, not a single packet came in."><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2025-11-18T00:00:00"><meta property="article:modified_time" content="2025-11-18T00:00:00"><meta property="article:tag" content="openwrt"><meta property="article:tag" content="flowtable"><meta name=twitter:card content="summary"><meta property="og:image" content="https://res.cloudinary.com/core2duoe6420/image/upload/v1643906248/DSC_5117_wviusa.jpg"><meta property="twitter:image" content="https://res.cloudinary.com/core2duoe6420/image/upload/v1643906248/DSC_5117_wviusa.jpg"><title>OpenWrt Hardware Flow Offload Causing MAC Address Caching</title><link rel=icon href=https://blog.hljin.net/favicon.png><link rel=canonical href=https://blog.hljin.net/en-us/2025/11/flowtable-mac-issue/><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css integrity="sha512-iBBXm8fW90+nuLcSKlbmrPcLa0OT92xO1BIsZ+ywDWZCvqsWgccV3gFoRBv0z+8dLJgyAHIhR35VZc2oM/gI1w==" crossorigin=anonymous referrerpolicy=no-referrer><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha512-H9jrZiiopUdsLpg94A333EfumgUBpO9MdbxStdeITo+KEIMaNfHNvwyjjDJb+ERPaRS6DpyRlKbvPUasNItRyw==" crossorigin=anonymous referrerpolicy=no-referrer><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.7.2/gitalk.css integrity="sha512-MLcK/YRapzET1qTBXrOiZE6bGBgtATMo2bIyalVJ8EKDEGNoeA3SPQkvWAR0zNS650YG13ocXBMeioDuZcSRuQ==" crossorigin=anonymous referrerpolicy=no-referrer><link rel=stylesheet href=https://blog.hljin.net/css/style-h6ccsoet3mzkbb0wngshlfbaweimexgqcxj0h5hu4h82olsdzz6wmqdkajm.min.css><link rel=stylesheet href=https://blog.hljin.net/css/override.css></head><body><div id=blog><header id=header data-behavior=4><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=https://blog.hljin.net/en-us/ aria-label="Go to homepage">Alex King's blog</a></div><a class=header-right-picture href=https://blog.hljin.net/#about aria-label="Open the link: /#about"><img class=header-picture src=https://res.cloudinary.com/core2duoe6420/image/upload/v1643906248/DSC_5117_wviusa.jpg alt="Author's picture"></a></header><nav id=sidebar data-behavior=4><div class=sidebar-container><div class=sidebar-profile><a href=https://blog.hljin.net/#about aria-label="Read more about the author"><img class=sidebar-profile-picture src=https://res.cloudinary.com/core2duoe6420/image/upload/v1643906248/DSC_5117_wviusa.jpg alt="Author's picture"></a><h4 class=sidebar-profile-name>Alex King</h4><h5 class=sidebar-profile-bio>Observing without evaluating is the highest form of human intelligence</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://blog.hljin.net/en-us/ title=Home><i class="sidebar-button-icon fas fa-lg fa-home" aria-hidden=true></i>
<span class=sidebar-button-desc>Home</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://blog.hljin.net/en-us/categories title=Categories><i class="sidebar-button-icon fas fa-lg fa-bookmark" aria-hidden=true></i>
<span class=sidebar-button-desc>Categories</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://blog.hljin.net/en-us/tags title=Tags><i class="sidebar-button-icon fas fa-lg fa-tags" aria-hidden=true></i>
<span class=sidebar-button-desc>Tags</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://blog.hljin.net/en-us/archives title=Archives><i class="sidebar-button-icon fas fa-lg fa-archive" aria-hidden=true></i>
<span class=sidebar-button-desc>Archives</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://blog.hljin.net/en-us/#about title=About><i class="sidebar-button-icon fas fa-lg fa-user" aria-hidden=true></i>
<span class=sidebar-button-desc>About</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/core2duoe6420 target=_blank rel=noopener title=GitHub><i class="sidebar-button-icon fab fa-lg fa-github" aria-hidden=true></i>
<span class=sidebar-button-desc>GitHub</span></a></li></ul><ul class=sidebar-buttons></ul></div></nav><div id=main data-behavior=4 class=hasCoverMetaIn><article class=post id=top><div class="post-header main-content-wrap text-left"><h1 class=post-title>OpenWrt Hardware Flow Offload Causing MAC Address Caching</h1><div class="postShorten-meta post-meta"><time datetime=2025-11-18T00:00:00Z>November 18, 2025</time></div></div><div class="post-content markdown"><div class=main-content-wrap><div class="alert warning"><p>This article is translated from Chinese to English by ChatGPT. There might be errors.</p></div><p>Ran into yet another pitfall. This time it happened when I tried to migrate my original Proxmox VM ImmortalWrt (a build of OpenWrt) into Docker while keeping the IP address unchanged. This ImmortalWrt instance is running my WireGuard service. The migration itself went smoothly. After starting the Docker container, ping worked fine, and my phone could connect to WireGuard. Only one always-on 24/7 node stubbornly refused to connect: no handshake, WireGuard showed 0 KB received, not a single packet came in.</p><p>I tried enabling WireGuard kernel logging:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>echo <span style=color:#e6db74>&#34;module wireguard +p&#34;</span> | sudo tee /sys/kernel/debug/dynamic_debug/control
</span></span></code></pre></div><p>But the logs only showed handshake failures due to timeouts, nothing more.</p><p>This container is running on the bridged host network I described <a href=../docker-host-bridge-network/>in the previous post</a>. The overall topology includes a bridge, a veth pair, and a macvlan with IP address <code>192.168.1.240</code>. I tried using tcpdump on each node along the path and found that replies did exist: on the parent interface of the macvlan I could always capture the response packets, but as soon as they entered the container they disappeared without a trace.</p><p>After racking my brains, I asked ChatGPT. It replied that if the MAC address doesn’t match any macvlan virtual NIC’s MAC, the packets would be dropped. My immediate reaction was: that’s impossible—ping works fine, and some devices can connect successfully, which implies ARP is working. It later turned out ChatGPT was right; this was indeed why WireGuard couldn’t connect. I wasted a lot of troubleshooting time until I was completely out of ideas and, as a last-ditch effort, compared MAC addresses and finally noticed something.</p><p>Previously when I ran tcpdump I hadn’t added the <code>-e</code> flag. With it, the mismatch between the source/destination MAC addresses in the two directions became obvious:</p><pre tabindex=0><code>$ sudo tcpdump -i veth-macvlan1 -n -vvvv port 51820 -e
tcpdump: listening on veth-macvlan1, link-type EN10MB (Ethernet), snapshot length 262144 bytes
13:16:12.183767 3a:ea:10:f9:a1:2e &gt; bc:24:11:2b:dd:87, ethertype IPv4 (0x0800), length 190: (tos 0x88, ttl 64, id 7184, offset 0, flags [none], proto UDP (17), length 176)
    192.168.1.240.51820 &gt; x.x.x.x.51820: [bad udp cksum 0x11d5 -&gt; 0xecab!] UDP, length 148
13:16:12.201030 bc:24:11:2b:dd:87 &gt; b6:01:52:44:16:fa, ethertype IPv4 (0x0800), length 134: (tos 0x0, ttl 52, id 32270, offset 0, flags [none], proto UDP (17), length 120)
    x.x.x.x.51820 &gt; 192.168.1.240.51820: [udp sum ok] UDP, length 92
</code></pre><p>This was very strange: all other connections worked, only this node had issues. So I focused on the main router that was sending these packets—another ImmortalWrt box. Since the problem only occurred on this one connection, I suspected conntrack again. On the main router I checked conntrack:</p><pre tabindex=0><code>root@r-main:~# conntrack -L | grep 51820
udp      17 src=x.x.x.x dst=192.168.0.254 sport=51820 dport=51820 packets=5418 bytes=2996332 src=192.168.1.240 dst=x.x.x.x sport=51820 dport=51820 packets=3649 bytes=1587316 [OFFLOAD] mark=0 use=2
conntrack v1.4.8 (conntrack-tools): 395 flow entries have been shown.
</code></pre><p>After deleting this entry, WireGuard immediately connected successfully. But once I recreated the container, it would fail again, and the received MAC address would become the MAC of the previous container. At this point the issue was reproducible: the trigger condition was a change in MAC address. So I could infer that something on the main router must be caching the MAC address. Checking the output of <code>ip neigh show</code> looked perfectly normal; there were no stale MAC addresses.</p><p>As far as I knew, nothing should cache MAC addresses for that long, so I started blindly searching online. The <code>[OFFLOAD]</code> flag in the conntrack log caught my eye. I’d never seen it on Ubuntu. Following that clue I found flowtable, a new nftables feature. You can read more in these two articles:</p><ul><li><a href=https://docs.kernel.org/networking/nf_flowtable.html>Netfilter’s flowtable infrastructure</a></li><li><a href="https://thermalcircle.de/doku.php?id=blog:linux:flowtables_1_a_netfilter_nftables_fastpath">Flowtables - Part 1: A Netfilter/Nftables Fastpath</a></li></ul><p>In the first kernel document, there’s this paragraph:</p><blockquote><p>The flowtable behaves like a cache. The flowtable entries might get stale if either the destination MAC address or the egress netdevice that is used for transmission changes.
This might be a problem if:</p><ul><li>You run the flowtable in software mode and you combine bridge and IP forwarding in your setup.</li><li>Hardware offload is enabled.</li></ul></blockquote><p>This matches my issue. In ImmortalWrt (I’m on 24.10.4), the relevant settings are in the firewall:</p><div class="figure center fig-50"><img class=fig-img src=https://res.cloudinary.com/core2duoe6420/image/upload/v1763485317/posts/flowtable-mac-issue/firewall_fc42gf.png></div><div style=clear:both></div><p>This “Traffic Offload Type” is the key. For some reason it was set to Hardware Flow Offloading. The i350-T4 NIC I’m using shouldn’t support this. Experiments showed that when set to “Hardware Flow Offloading”, nftables would have:</p><pre tabindex=0><code>flowtable ft {
    hook ingress priority filter
    devices = { eth0, eth1, eth2, eth3, eth4 }
    flags offload
    counter
}

chain forward {
    type filter hook forward priority filter; policy drop;
    meta l4proto { tcp, udp } flow add @ft
    ...
}
</code></pre><p>Whereas when set to “Software Flow Offloading”, it would have:</p><pre tabindex=0><code>flowtable ft {
    hook ingress priority filter
    devices = { br-lan, eth4 }
    counter
}
</code></pre><p>In theory, setting this to “Hardware Flow Offloading” shouldn’t actually take effect. According to the documentation, if hardware offload is active, conntrack entries should show <code>HW_OFFLOAD</code>, not <code>OFFLOAD</code>. On my system, whether I chose software or hardware, conntrack always showed <code>OFFLOAD</code>. The weird part is that when set to hardware, long-lived UDP connections (WireGuard keeps trying to handshake every few seconds, so this UDP flow never times out in conntrack) would encounter this MAC address caching issue, but when set to software, the problem disappeared.</p></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small">TAGGED IN</span><br><a class="tag tag--primary tag--small" href=https://blog.hljin.net/en-us/tags/openwrt/>openwrt</a>
<a class="tag tag--primary tag--small" href=https://blog.hljin.net/en-us/tags/flowtable/>flowtable</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://blog.hljin.net/en-us/2025/11/pve-efi-boot-loader/ data-tooltip="Fixing 'unable to install the EFI boot loader' when installing PVE" aria-label="NEXT: Fixing 'unable to install the EFI boot loader' when installing PVE"><i class="fa fa-angle-left"></i>
<span class="hide-xs hide-sm text-small icon-ml">NEXT</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://blog.hljin.net/en-us/2025/11/docker-host-bridge-network/ data-tooltip="Docker Bridging to Host Network" aria-label="PREVIOUS: Docker Bridging to Host Network"><span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions aria-label="Share this post"><i class="fa fa-share-alt" aria-hidden=true></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#gitalk aria-label="Leave a comment"><i class="fa fa-comment"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#top aria-label="Back to top"><i class="fa fa-arrow-up" aria-hidden=true></i></a></li></ul></div><div id=gitalk><noscript>Please enable JavaScript to view the comments powered by Gitalk.</noscript></div><script src=https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.7.2/gitalk.min.js integrity="sha512-EcTCcXV46teiNwe0VcnM5A038tcY+BaQYO4nW6Gh2i7v4/HjBVg7xx3+JBLl9WofDds//INJAiEGAtdgr8PWyA==" crossorigin=anonymous referrerpolicy=no-referrer></script>
<script type=text/javascript>(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("gitalk").innerHTML="Gitalk comments not available by default when the website is previewed locally.";return}new Gitalk({clientID:"2d9c9d837f8496107211",clientSecret:"5a474517d0eb6d5abdb90ab9a31c2ba94f4f43b2",repo:"core2duoe6420.github.io",owner:"core2duoe6420",admin:["core2duoe6420"],id:"590e08aa5db292db528a2cf600a40b89",...{distractionfreemode:!1,enablehotkey:!0,language:"zh-CN",pagerdirection:"first",perpage:10}}).render("gitalk")})()</script></div></article><footer id=footer class=main-content-wrap><span class=copyrights>&copy; 2025 Powered by Hugo with tranquilpeak. All Rights Reserved</span></footer></div><div id=bottom-bar class=post-bottom-bar data-behavior=4><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://blog.hljin.net/en-us/2025/11/pve-efi-boot-loader/ data-tooltip="Fixing 'unable to install the EFI boot loader' when installing PVE" aria-label="NEXT: Fixing 'unable to install the EFI boot loader' when installing PVE"><i class="fa fa-angle-left"></i>
<span class="hide-xs hide-sm text-small icon-ml">NEXT</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://blog.hljin.net/en-us/2025/11/docker-host-bridge-network/ data-tooltip="Docker Bridging to Host Network" aria-label="PREVIOUS: Docker Bridging to Host Network"><span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions aria-label="Share this post"><i class="fa fa-share-alt" aria-hidden=true></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#gitalk aria-label="Leave a comment"><i class="fa fa-comment"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#top aria-label="Back to top"><i class="fa fa-arrow-up" aria-hidden=true></i></a></li></ul></div></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-times"></i></div><img id=about-card-picture src=https://res.cloudinary.com/core2duoe6420/image/upload/v1643906248/DSC_5117_wviusa.jpg alt="Author's picture"><h4 id=about-card-name>Alex King</h4><div id=about-card-bio>Observing without evaluating is the highest form of human intelligence</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Human</div><div id=about-card-location><i class="fa fa-map-marker-alt"></i><br>Shanghai</div></div></div><div id=cover style=background-image:url(https://res.cloudinary.com/core2duoe6420/image/upload/v1643905455/20220204002352_yqvhwd.jpg)></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js integrity="sha512-894YE6QWD5I59HgZOGReFYm4dnWc1Qt5NtvYSaNcOP+u1T9qYdvdihz0PPSiiqn/+/3e7Jo4EaG7TubfWGUrMQ==" crossorigin=anonymous referrerpolicy=no-referrer></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/highlight.min.js integrity="sha512-z+/WWfyD5tccCukM4VvONpEtLmbAm5LDu7eKiyMQJ9m7OfPEDL7gENyDRL3Yfe8XAuGsS2fS4xSMnl6d30kqGQ==" crossorigin=anonymous referrerpolicy=no-referrer></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha512-uURl+ZXMBrF4AwGaWmEetzrd+J5/8NRkWAvJx5sbPSSuOb0bZLqf+tOzniObO00BjHa/dD7gub9oCGMLPQHtQA==" crossorigin=anonymous referrerpolicy=no-referrer></script>
<script src=https://blog.hljin.net/js/script-yqzy9wdlzix4lbbwdnzvwx3egsne77earqmn73v9uno8aupuph8wfguccut.min.js></script>
<script async crossorigin=anonymous defer integrity="sha512-gE8KAQyFIzV1C9+GZ8TKJHZS2s+n7EjNtC+IMRn1l5+WYJTHOODUM6JSjZhFhqXmc7bG8Av6XXpckA4tYhflnw==" src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/apache.min.js></script>
<script async crossorigin=anonymous defer integrity="sha512-EWROca+bote+7Oaaar1F6y74iZj1r1F9rm/ly7o+/FwJopbBaWtsFDmaKoZDd3QiGU2pGacBirHJNivmGLYrow==" src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/go.min.js></script>
<script async crossorigin=anonymous defer integrity="sha512-GDVzAn0wpx1yVtQsRWmFc6PhJiLBPdUic+h4GWgljBh904O3JU10fk9EKNpVyIoPqkFn54rgL2QBG4BmUTMpiQ==" src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/http.min.js></script>
<script async crossorigin=anonymous defer integrity="sha512-UgZlma8NzkrDb/NWgmLIcTrH7i/CSnLLDRFqCSNF5NGPpjKmzyM25qcoXGOup8+cDakKyaiTDd7N4dyH4YT+IA==" src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/less.min.js></script>
<script async crossorigin=anonymous defer integrity="sha512-lot9koe73sfXIrUvIPM/UEhuMciN56RPyBdOyZgfO53P2lkWyyXN7J+njcxIIBRV+nVDQeiWtiXg+bLAJZDTfg==" src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/nginx.min.js></script>
<script async crossorigin=anonymous defer integrity="sha512-Zd3e7XxHP00TD0Imr0PIfeM0fl0v95kMWuhyAS3Wn1UTSXTkz0OhtRgBAr4JlmADRgiXr4x7lpeUdqaGN8xIog==" src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/puppet.min.js></script>
<script async crossorigin=anonymous defer integrity="sha512-qtqDO052iXMSP+5d/aE/jMtL9vIIGvONgTJziC2K/ZIB1yEGa55WVxGE9/08rSQ62EoDifS9SWVGZ7ihSLhzMA==" src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/scss.min.js></script>
<script async crossorigin=anonymous defer integrity="sha512-1NmkjnEDnwwwcu28KoQF8vs3oaPFokQHbmbtwGhFfeDsQZtVFI8zW2aE9O8yMYdpdyKV/5blE4pSWw4Z/Sv97w==" src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/stylus.min.js></script>
<script async crossorigin=anonymous defer integrity="sha512-B2wSfruPjr8EJL6IIzQr1eAuDwrsfIfccNf/LCEdxELCgC/S/ZMt/Uvk80aD79m7IqOqW+Sw8nbkvha20yZpzg==" src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/swift.min.js></script>
<script async crossorigin=anonymous defer integrity="sha512-28oDiQZGKUVN6wQ7PSLPNipOcmkCALXKwOi7bnkyFf8QiMZQxG9EQoy/iiNx6Zxj2cG2SbVa4dXKigQhu7GiFw==" src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/yaml.min.js></script>
<script async crossorigin=anonymous defer src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
<script>$(document).ready(function(){hljs.configure({classPrefix:"",useBR:!1}),$("pre.code-highlight > code, pre > code").each(function(e,t){$(this).hasClass("codeblock")||$(this).addClass("codeblock"),hljs.highlightBlock(t)})})</script></body></html>